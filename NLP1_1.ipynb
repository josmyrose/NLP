{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdwNUX9a6WNYiPCntOE5ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josmyrose/NLP/blob/main/NLP1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP BASICS"
      ],
      "metadata": {
        "id": "ybImhfhjvIUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simple example of text processing in Python using the NLTK (Natural Language Toolkit) library, which is a popular library for NLP tasks. For this example, we'll perform basic text preprocessing and calculate the word frequency in a sample text.\n",
        "\n",
        "Install nltk"
      ],
      "metadata": {
        "id": "QGpjSiYPu-cn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K62_1e4uu7ko",
        "outputId": "bb000f9a-13b3-496b-b938-b9188017cc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create simple python script to create text processing"
      ],
      "metadata": {
        "id": "czCT8hv7xzwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries"
      ],
      "metadata": {
        "id": "uYUT_bd8yJyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from nltk.corpus import stopwords\n",
        "This python code used for import module named stopword from nltk library.NLTK is a popular Python library used for Natural Language Processing (NLP) tasks.\n",
        "\n",
        "stopwords are words that are commonly used in languages,but it does not contribute much to the overall meaning of a sentences.Example for the stopwords in english are \"the,\" \"is,\" \"and,\" \"in,\" \"of,\" \"a,\" \"an,\" etc.These words frequently appear in all texts and removed these words during text processing to reduce noise and focus on more meaningful words."
      ],
      "metadata": {
        "id": "b7ZV8AaiygUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stopwords module in NLTK provides a list of stopwords for various languages, including English. It is a useful resource for text processing tasks as it allows us to easily access and utilize this list of common stopwords."
      ],
      "metadata": {
        "id": "jopWlThf1mv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "\n"
      ],
      "metadata": {
        "id": "Zc-pn4ZKwd_8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hTUwpiVg1yUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjBj9n4Y1h28",
        "outputId": "b99a14f5-9050-4c08-9b9d-c14201f963f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "text_data = \"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is useful and meaningful. There are various approaches and techniques in NLP, but here are five essential steps commonly involved in many NLP tasks: Text Preprocessing, Lexical Analysis, Syntactic Analysis, Semantic Analysis, and Pragmatic Analysis.\""
      ],
      "metadata": {
        "id": "u07v9JdF1mUM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    print(\"Preprocessed Tokens:\",tokens)\n",
        "    # Removing stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "Db9H7mLH2ndn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expression [token for token in tokens if token.isalnum() and token not in stop_words] is a list comprehension used in the Python code to filter and process the tokens obtained during text preprocessing.\n",
        "\n",
        "Let's break down the expression step by step:\n",
        "\n",
        "token for token in tokens: This is the basic structure of a list comprehension. It iterates over each element (token) in the tokens list and applies some operation to it.\n",
        "\n",
        "if token.isalnum(): This is a condition applied to each token in the list comprehension. The isalnum() method is a built-in string method in Python that checks if the token contains only alphanumeric characters (letters and digits) and has at least one character. This condition ensures that we only keep tokens that consist of letters and/or digits and remove any tokens containing special characters or punctuation.\n",
        "\n",
        "and token not in stop_words: This is another condition applied to each token in the list comprehension. It checks if the token is not present in the stop_words set. The stop_words set contains common stopwords, as obtained from the NLTK library. This condition ensures that we remove any stopwords from the list of tokens, as stopwords generally do not carry significant meaning in the context of NLP tasks.\n",
        "\n",
        "So, the overall effect of this list comprehension is to create a new list (filtered_tokens) that contains only alphanumeric tokens (words and numbers) that are not present in the stop_words set. Essentially, it removes all the non-alphanumeric tokens (like punctuation) and stopwords from the original list of tokens, resulting in a list of meaningful words that can be used for further analysis or processing in NLP tasks.\n"
      ],
      "metadata": {
        "id": "v9WLgNy93t-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Calculate word frequency\n",
        "def calculate_word_frequency(tokens):\n",
        "    word_freq = Counter(tokens)\n",
        "    return word_freq\n",
        "\n",
        "# Process the text data\n",
        "preprocessed_tokens = preprocess_text(text_data)\n",
        "word_frequency = calculate_word_frequency(preprocessed_tokens)\n",
        "\n",
        "# Display the results\n",
        "print(\"Preprocessed Tokens:\", preprocessed_tokens)\n",
        "print(\"Word Frequency:\", word_frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wED34Jg-3Q5D",
        "outputId": "226f65a2-fc78-45c7-ba1b-5131f2235eb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Tokens: ['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'the', 'goal', 'of', 'nlp', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'useful', 'and', 'meaningful', '.', 'there', 'are', 'various', 'approaches', 'and', 'techniques', 'in', 'nlp', ',', 'but', 'here', 'are', 'five', 'essential', 'steps', 'commonly', 'involved', 'in', 'many', 'nlp', 'tasks', ':', 'text', 'preprocessing', ',', 'lexical', 'analysis', ',', 'syntactic', 'analysis', ',', 'semantic', 'analysis', ',', 'and', 'pragmatic', 'analysis', '.']\n",
            "Preprocessed Tokens: ['natural', 'language', 'processing', 'nlp', 'field', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'human', 'language', 'goal', 'nlp', 'enable', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'way', 'useful', 'meaningful', 'various', 'approaches', 'techniques', 'nlp', 'five', 'essential', 'steps', 'commonly', 'involved', 'many', 'nlp', 'tasks', 'text', 'preprocessing', 'lexical', 'analysis', 'syntactic', 'analysis', 'semantic', 'analysis', 'pragmatic', 'analysis']\n",
            "Word Frequency: Counter({'nlp': 4, 'analysis': 4, 'language': 3, 'computers': 2, 'human': 2, 'natural': 1, 'processing': 1, 'field': 1, 'artificial': 1, 'intelligence': 1, 'focuses': 1, 'interaction': 1, 'goal': 1, 'enable': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'way': 1, 'useful': 1, 'meaningful': 1, 'various': 1, 'approaches': 1, 'techniques': 1, 'five': 1, 'essential': 1, 'steps': 1, 'commonly': 1, 'involved': 1, 'many': 1, 'tasks': 1, 'text': 1, 'preprocessing': 1, 'lexical': 1, 'syntactic': 1, 'semantic': 1, 'pragmatic': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in preprocessed_tokens:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    print(f\"{word} -> {stemmed_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro3esw537vPE",
        "outputId": "6b5d8e1b-782c-4c26-983b-36d4e98bbede"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural -> natur\n",
            "language -> languag\n",
            "processing -> process\n",
            "nlp -> nlp\n",
            "field -> field\n",
            "artificial -> artifici\n",
            "intelligence -> intellig\n",
            "focuses -> focus\n",
            "interaction -> interact\n",
            "computers -> comput\n",
            "human -> human\n",
            "language -> languag\n",
            "goal -> goal\n",
            "nlp -> nlp\n",
            "enable -> enabl\n",
            "computers -> comput\n",
            "understand -> understand\n",
            "interpret -> interpret\n",
            "generate -> gener\n",
            "human -> human\n",
            "language -> languag\n",
            "way -> way\n",
            "useful -> use\n",
            "meaningful -> meaning\n",
            "various -> variou\n",
            "approaches -> approach\n",
            "techniques -> techniqu\n",
            "nlp -> nlp\n",
            "five -> five\n",
            "essential -> essenti\n",
            "steps -> step\n",
            "commonly -> commonli\n",
            "involved -> involv\n",
            "many -> mani\n",
            "nlp -> nlp\n",
            "tasks -> task\n",
            "text -> text\n",
            "preprocessing -> preprocess\n",
            "lexical -> lexic\n",
            "analysis -> analysi\n",
            "syntactic -> syntact\n",
            "analysis -> analysi\n",
            "semantic -> semant\n",
            "analysis -> analysi\n",
            "pragmatic -> pragmat\n",
            "analysis -> analysi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsDFw5Mr7u-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}